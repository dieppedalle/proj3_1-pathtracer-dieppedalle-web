<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 284A Ray Tracing</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 284A: Computer Graphics and Imaging, Spring 2018</h1>
<h1 align="middle">Project 3-1: Ray Tracing</h1>
<h2 align="middle">Gauthier Dieppedalle, CS199-btx</h2>

<br><br>

<div>
  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<h2 align="middle">Overview</h2>
<p>In this project, I implemented a path tracing algorithm to render scenes with light. We will be implementing direct and then global illumination for dae files. This assignment will also be using the techniques that we have seen in lecture to optimize the way ray tracing works such as using bounding volume hierrachy and adaptive sampling.</p>

<h2 align="middle">Task I: Ray Generation and Scene Intersection</h2>

<h3 align="middle">Part 1: Filling in the sample loop</h3>
In this first part, I implemented the <code>PathTracer::raytrace_pixel()</code> function in <code>pathtracer.cpp</code>. The function creates $ns\_aa$ samples per pixel and traces a ray for each of those samples. In other words, the algorithm is generating $ns\_aa$ camera rays for a given pixel and then returns the average spectrum from these rays. The ray is generated using the <code>generate_ray</code> function, while the spectrum is calculated using the <code>est_radiance_global_illumination</code>. If the number of samples is 1, then we need to generate the ray through the center of the pixel. The location to the camera that is passed in to the generated ray has been scaled down to $[0,1]^2$ coordinates by dividing the $x$ and $y$ coordinate of the sample by the width and height of the pixel.

<h3 align="middle">Part 2: Generating camera rays</h3>
<p>In this section, I implemented the <code>Camera::generate_ray()</code> in <code>camera.cpp</code>. The function computes the position of the input sensor coordinate on the canonical sensor plane one unit away from the pinhole. As the camera is positioned at the origin, looks along the $âˆ’z$ axis, has the $+y$ axis as image space "up", we can define the sensor plane to have a bottom left corner of <code>Vector3D(-tan(radians(hFov)*.5), -tan(radians(vFov)*.5),-1)</code> and an upper right corner of <code>Vector3D( tan(radians(hFov)*.5),  tan(radians(vFov)*.5),-1)</code>. We then convert the input point to a point in the sensor coordinate. After, we convert the ray's direction in camera space and return the ray generated by that camera along the ray's direction calculated.</p>

<p>The returned value from <code>Camera::generate_ray()</code> is a ray object. A ray object takes in the origin of the ray and the direction of the ray. The ray is coming from the camera, so the origin is the position of the camera.</p>

<h3 align="middle">Part 3: Intersecting Triangles</h3>
<p>I then implemented the function <code>Triangle::intersect()</code> in <code>static_scene/triangle.cpp</code>. To detect the intersection of a ray with a triangle, I decided to use the Moller-Trumbore intersection algorithm. This algorithm is now often used to compute the intersection as it doesn't require too many calculations compared to other intersection algorithms.</p>
<p>To detect whether a ray intersects with a triangle using the Moller-Trumbore algorithm, we represent the intersecting point in barycentric coordinates:</p>
\[
P=wA+uB+vC
\]
<p>where $w$, $u$, and $v$ are the barycentric coordinates of $A$, $B$, and $C$, which are the 3 vertices of the triangle we want to check the intersection.</p> 
<p>We then know that $w=1-u-v$. Therefore, we can substitute these values in the equation above to get the following equations:</p>
\[
\begin{align}
P&=(1-u-v)A+uB+vC\\
&=A-uA-vA+uB+vC\\
&=A+u(B-A)+v(C-A)
\end{align}
\]
<p>Therefore, we now have the position of the intersecting point according to the barycentric coordinates $u$, $v$, the edges AB and AC of triangle ABC, and of the vertex A of the triangle ABC.</p>

<p>From the ray, we can also represent the intersection point as:</p>
\[
P = O + tD
\]
<p>where $t$ is the distance between $P$ and the ray's origin, $D$ is the direction of the ray, and $O$ is the origin of the ray.</p>

<p>Then if we replace $P$ in the equation above we get the following equation:</p>
\[
\begin{align}
&O + tD=A+u(B-A)+v(C-A)\\
\Leftrightarrow &O-A=-tD+u(B-A)+v(C-A)
\end{align}
\]
<p>Therefore in the equation above we have three unknowns ($t$, $u$, $v$) and three known terms (edge AB, edge AC, and direction D). Therefore, we can rearrange the equation as following:</p>
\[
\begin{bmatrix}
    -D       & e_1 & e_2
\end{bmatrix}
\begin{bmatrix}
    t \\
    u \\
    v
\end{bmatrix}
\]
<p>where $e_1=(B-A)$ and $e_2=(C-A)$</p>
<p>By using Cramer's rule, we can then write the equation above as:</p>
\[
\begin{bmatrix}
    t \\
    u \\
    v
\end{bmatrix}=\frac{1}{\begin{bmatrix}\begin{vmatrix}
    -D       & e_1 & e_2
\end{vmatrix}\end{bmatrix}}

\begin{bmatrix}
\begin{vmatrix}
    T       & e_1 & e_2
\end{vmatrix}\\
\begin{vmatrix}
    -D       & T & e_2
\end{vmatrix}\\
\begin{vmatrix}
    -D       & e_1 & T
\end{vmatrix}
\end{bmatrix}
\]

<p>Cramer's rule is an explicit formula for solving a system of linear equations and expresses the solution in terms of a determinant of the matrix and a matrix made up of the columns of the 1x3 matrix. </p>

<p>By rearranging the terms we get,</p>
\[
\begin{bmatrix}
    t \\
    u \\
    v
\end{bmatrix}=\frac{1}{(D \times e_2)\cdot e_1}

\begin{bmatrix}
(T \times e_1) \cdot e_2\\
(D \times e_2) \cdot T\\
(T \times e_1) \cdot D
\end{bmatrix}
\]

<p>I therefore implemented the above equation in my code and determined that the ray intersect the triangle if $t$ is between $min\_t$ and $max\_t$ (valid range of $t$), $u$ is between 0 and 1, and $v$ is between 0 and 1, and $u+v$ is less than 1.</p>

<h3 align="middle">Part 4: Intersecting Spheres</h3>
<p>In this part I implemented <code>Sphere::intersect()</code> in <code>static_scene/sphere.cpp</code>. In order to test for intersection I solved the following equation via code:</p>
\[
at^2+bt+c=0
\]
<p>where $a= d \cdot d$, $b=2(o-c)\cdot d$, $c=(o-c) \cdot (o-c)-R^2$.</p>

<p>In which $d$ is the ray's direction, $o$ is the origin of the ray, $c$ is the center of the sphere, and $R$ is the radius of the sphere.</p>

<p>The solutions to a quadratic equations are</p>
\[
x_1=\frac{-b-\sqrt{b^2-4ac}}{2a} \text{ and } x_2=\frac{-b+\sqrt{b^2-4ac}}{2a}
\]

<p>In order for there to be a solution we need to have the discrimant of the quadratic equation greater or equal to zero. When there is no solution the <code>test</code> function must return false. When there is only one solution (determinant is equal to zero) then we need to assign that solution to $t1$ and $t2$. One solution means that the ray is tangent to the sphere. When there is two solutions we assign the two solutions to $t1$ and $t2$ but make sure that the smaller of the two is assigned to $t1$. We also need to make sure that each of the $t$ values are within the $min\_t$ and $max\_t$ boundaries for that $t$ to be a valid solution.</p>

<p>The normal shading gives the following images on a few sample dae files:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/spherePart1.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 2.2719s.</figcaption>
      </td>
      <td>
        <img src="images/gemsPart1.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBgems.dae</code> rendered in 33.7194s.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/emptyPart1.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBempty.dae</code> rendered in 2.0011s.</figcaption>
      </td>
    </tr>
  </table>
</div>

<h2 align="middle">Task II: Bounding Volume Hierarchy</h2>
<p>In this part I implemented the bounding volume hierarchy to accelerate the ray tracing in more complex scenes. This technique is implemented by placing each object in the scene in a binary tree containing rectangular bounding boxes. From this data structure, we will be able to make our ray intersection algorithm run faster as we will only check ray intersection between bounding boxes instead of every single primitive in the scene.</p>

<p>By running the algorithm without optimization on <code>cow.dae</code>, the scene is rendered in about 162s.</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/cowPart1.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> rendered in 70.1354s.</figcaption>
      </td>
      <td>
        <img src="images/head.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/maxplanck.dae</code> rendered in 736.7218s.</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3 align="middle">Part 1: Constructing the BVH</h3>
<p>In this part, I implemented the function <code>BVHAccel:construct_bvh()</code> inside <code>bvh.cpp</code> to build the BVH. The function takes in a vector of primitives and a maximum leaf size configuration and needs to return the BVH tree created.</p>
<p> The algorithm first computes the bounding box of the primitives in prims in a loop by calling the <code>get_bbox()</code> function on each primitive and assigning it to the <code>bbox</code> variable. We also compute the centroid of each box that we assign to the <code>centroid_box</code> variable. For each bounding box, we create a BVH node. Then we check if there are at most $max\_leaf\_size$ primitives in the primitive list and if it is the case then this is a leaf node. We therefore simply return that node with the list of primitives assigned to it. If it is not the case we then need to seperate the primitives among the left and right children nodes. To decide on which side the primitives should go in the tree, we check whether their bounding box's centroid's coordinate in the chosen axis is less than or greater than the split point. The axis is selected by choosing the longest axis if the centroid bounding box. In other words, if the extent of $x$ is greater then the extent of $y$ and $z$ then the split must happen on $x$. If the extent of $y$ is greater then the extent of $x$ and $z$ then we split on $y$. If both cases are not true then we split on the $z$ axis. A heuristic fpr split is considered to be good if it reduces the time to check ray intersections. The heuristic that I implemented works well but could be further optimized.</p>
<p>We then split all the primitives into two vectors  based on whether their bounding box's centroid's coordinate in the chosen axis is less than or greater than the split point. If both vectors have elements inside of them meaning that there are bounding boxes to the left and right of the choosen axis then we assign the left and right children nodes to these vectors. If one of the vectors do not contain any element then we need to split the other vector into two vectors by dividing the vector into 2 vectors in the middle and then assign the two generated lists to the left and right node. We then return the BVH node that we have created.</p>

<h3 align="middle">Part 2: Intersecting BBox</h3>
<p>In this part, I implemented the <code>BVHAccel::intersect()</code> routines inside <code>bvh.cpp</code>. According to the slides, the intersecting point of a plane with a ray can be obtained from the following equation:</p>
\[
t=\frac{(p'-o) \cdot N}{d \cdot N}
\]
<p>in which $p'$ is a point on the plane, $o$ is the origin of the ray, $N$ is the normal vector of the plane, and $d$ is the direction of the ray.</p>
<p>In <code>BVHAccel::intersect()</code>, I am first calculating the values of $t$ in each axis, so $x$, $y$, and $z$, using a derivation of the above formula that you can find below:</p>
\[
t=\frac{p'_x-o_x}{d_x}
\]
<p>A ray misses a box when <code>tmin > tmax</code> where:</p>
<p><code>double tmin = std::max(std::max(txmin, tymin), tzmin);</code></p>
<p><code>double tmax = std::min(std::min(txmax, tymax), tzmax);</code></p>
<p>If we are not missing the bounding box, then we return true and assign <code>t0</code> to the minimum <code>t</code> value and <code>t1</code> to the maximum <code>t</code> value.</p>

<h3 align="middle">Part 3: Intersecting BVHAccel</h3>
<p>In this section, I implemented the two <code>BVHAccel::intersect()</code> routines inside <code>bvh.cpp</code>. The function takes a ray and a BVH node. First, we check if the given ray misses the bounding box. If it is the case then we return false immediately. We also return false if <code>t</code> interval has an empty intersection with the ray's interval from <code>min_t</code> to <code>max_t</code>. If the current node is a leaf node then we test intersections with all primtives in that node and return the closest intersection. If the current node is not a leaf node then we recursively check for intersection for the left and right children nodes and return true if there is an intersection and false otherwise.</p>

<p>The bounding box hierarchy can be seen in the following screenshots:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/cow0.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> initial bounding box.</figcaption>
      </td>
      <td>
        <img src="images/cow1.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> pressing right key.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/cow2.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> pressing right key.</figcaption>
      </td>
      <td>
        <img src="images/cow3.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> pressing right key.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/cow4.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> pressing right key.</figcaption>
      </td>
      <td>
        <img src="images/cow5.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> pressing right key.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/cow6.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> pressing right key.</figcaption>
      </td>
      <td>
        <img src="images/cow7.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> pressing right key.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/cow8.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> pressing right key.</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>Once the BVH is working, we are able to do normal shading for a large dae files that we couldn't render without the acceleration structure. The following are a couple of dae files using the BVH that we just implemented:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/cowPart2.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/cow.dae</code> rendered in 0.0285s.</figcaption>
      </td>
      <td>
        <img src="images/maxplanckPart2.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/meshedit/maxplanck.dae</code> rendered in 0.3187s.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/CBlucyPart2.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBlucy.dae</code> rendered in 1.0664s.</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>Through the images above we can see that the rendering is much faster using the BVH technique rather than checking the ray intersection for each primitive. The <code>cow.dae</code> file without any optimizations took 70.1354s to compile, however with BVH it can render in 0.0285s. This is therefore a speed improvement of $\frac{70.1354}{0.0285}=2460$ times. For <code>dae/meshedit/maxplanck.dae</code>, the scene renders in 736.7218s without any bounding boxes but otherwise renders in 0.3187s. This is therefore a speed improvement of $\frac{736.7218}{0.3187}=2311$ times. This speed improvement makes complete sense because we are now checking the intersections of a lot less objects than before. Now we are only checking the intersection of bounding boxes instead of the complex geometry of every single primitives. The geometry of a bounding box is easier to represent and therefore easier to check for intersection with a ray using the intersecting technique used above (we simply need to check if the ray intersects with each set of line in the bounding box). The scene graph that we generated is also a very efficient way to iterate through the primitives of a scene. I was not able to render <code>CBlucy.dae</code> in the non-optimized ray tracing as it took way too long to render on the instructional machines.</p>

<h2 align="middle">Task III: Direct Illumination</h2>
<h3 align="middle">Part 1: Diffuse BSDF</h3>
<p>In this part, I had to implement the <code>DiffuseBSDF::f</code> and <code>DiffuseBSDF::sample_f</code> in <code>bsdf.cpp</code>.</p> 
<p>For the <code>f</code> function, according to the slide for a diffuse surface the bsdf is $\frac{reflectance}{\pi}$.</p>
<p>For the <code>sample_f</code> function, I simply get a sample that I assign to <code>wi</code> and then return the value from the <code>f</code> function that I just implemented.</p>

<h3 align="middle">Part 2: Direct lighting</h3>
<p>In this part I had to implement <code>estimate_direct_lighting_hemisphere</code> in which we apply direct lighting on a point by sampling uniformly in a hemisphere and <code>estimate_direct_lighting_importance</code> in which we instead use importance sampling by sampling all the lights directly.</p>
<p>For <code>estimate_direct_lighting_hemisphere</code> we sample uniformly over a hemisphere around the point <code>hit_p</code>. For each ray intersecting a light source, we compute the incoming radiance of that light source that we convert into an outgoing radiance. From all of these radiances we average over all the samples and return the average radiance.</p>
<p>To be more specific, the function takes in a ray and an intersection point. We first make a coordinate system for a hit point with the normal vector aligned with the Z direction. This is needed because the BSDF functions require the normal to be (0,0,1). The hit point can be calculated as following using the ray equation <code>hit_p = r.o + r.d * isect.t</code>. The outgoing direction can be calculated as the opposite to the direction that the ray was traveling. The number of samples taken will be <code>scene->lights.size() * ns_area_light</code> to have the same total number of samples then <code>estimate_direct_lighting_importance</code>. We then iterate through each sample. A sample can be obtained using the function <code>get_sample</code>. We then compute the bsdf using the <code>f</code> function and transform the direction of the ray to world space using the following equation <code>wi_world = o2w * wi</code>. A ray is then created using these parameters created and check for intersections using the <code>intersect</code> function of the <code>bvh</code>. If it hits something we accumulate the total spectrum by multiplying the intersected material's emitted light with the bsdf calculate as well as multipying by the cosine of the angle between a direction vector w_in and the normal vector and multiply by the PDF which is $2\pi$ for a hemishphere that has uniform distribution. We then divide the total accumulated spectrum by the number of samples and return that value. </p> 
<p>For importance sampling, we sum over each light source in the scene by sampling each point on the light source and then we compute the radiance from the sampled directions. As in hemisphere sampling, we then convert the incoming radiance to an outgoing radiance using the bsdf. The main difference between the two techniques is where we take the samples in the scene. In hemisphere we sample over a hemisphere while in importance light sampling we sample over the points of the light source.</p>
<p>When we iterate over each light source, we check if the light source is a delta light. If it is the case then we only sample once as all the samples would be the same. We then take all the samples for a given light source by calling the <code>sample_l</code> function. If the z coordinate of <code>w_in</code> is negative then we can continue the loop since we know the sampled light point lies behind the surface. Then we cast a ray from the intersection point to towards the light to see if it intersects any point in the scene. The <code>max_t</code> will be the distance to the light as we do not care about intersections behind the light source. If the ray doesn't intersect the scene then we can calculate the bsdf value the bsdf value at <code>w_out</code> and <code>w_in</code> multiply by the cosine of the angle between a direction vector w_in and the normal vector and divide by the pdf to find all of the outgoing radiances. Once we have found all the outgoing radiances we sum them up and then divide by the number of samples to get the average outgoing radiance.</p>
<p>For uniform hemisphere sampling we obtain the following image using the command line <code>./pathtracer -t 8 -s 16 -l 8 -m 6 -H -f CBbunny_16_8.png -r 480 480 ../dae/sky/CBbunny.dae</code> and <code>./pathtracer -t 8 -s 64 -l 32 -m 6 -H -f CBbunny_64_32.png -r 480 360 ../dae/sky/CBbunny.dae</code>:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/Part2CBbunny_16_8.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered in 13.0623s.</figcaption>
      </td>
      <td>
        <img src="images/Part2CBbunny_64_32.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered in 192.8449s.</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>For uniform importance sampling we obtain the following image by running the command line <code>./pathtracer -t 8 -s 64 -l 32 -m 6  -f dragon_64_32.png -r 480 480 ../dae/sky/dragon.dae</code> and <code>./pathtracer -t 8 -s 64 -l 32 -m 6  -f bunny_64_32.png -r 480 360 ../dae/sky/CBbunny.dae</code>:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/dragon_64_32.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/dragon.dae</code> rendered in 138.9783s.</figcaption>
      </td>
      <td>
        <img src="images/bunny_64_32.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered in 162.1165s.</figcaption>
      </td>
    </tr>
  </table>
</div>

<p>We can see that the images generated using hemisphere sampling are a lot more noisy then the images generated using uniform importance sampling. The images generated using hemisphere sampling are also slightly darker than the ones generated with importance sampling. Hemisphere sampling is also much slower than importance sampling as evaluating all the points on the hemisphere of each hit point is expensive. Additionally we can see that the errors generated in the hemishphere sampling image have pixels that are either too bright or too dark. The noisness in hemisphere sampling is due to the fact that we are sampling rays using the <code>get_sample</code> function over a hemisphere. The rays sampled over the hemisphere may not be intersecting with the light. When there is a low sample rate as a result, these rays missing the light are therefore causing black dots in the image because a lot of these rays are missing the light. To summarize the hemisphere sampling is sampling over the brdf by generating a hemisphere. The direct importance sampling samples over the light source. As a result, each ray in direct importance sampling has a light and the rendered image is smooth. Using hemisphere sampling, we can't render scenes with point light sources (like <code>bunny.dae</code> and <code>dragon.dae</code>), since our outgoing rays will never intersect with the meshes.</p>

<p>For uniform importance sampling by varying the number of light rays with 1, 4, 16, and 64 light rays (the -l flag) and 1 sample per pixel (the -s flag) :</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/bunny_1_1.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered in 0.1204s with 1 light ray.</figcaption>
      </td>
      <td>
        <img src="images/bunny_1_4.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered in 0.3262s with 4 light ray.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/bunny_1_16.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered in 1.1790s with 6 light ray.</figcaption>
      </td>
      <td>
        <img src="images/bunny_1_64.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered in 4.6184s with 64 light ray.</figcaption>
      </td>
    </tr>
  </table>
</div>

<p>We can see through these images that if we take more sample then the amount of noise is going to get reduced. The shadows are also softer when using more rays (as it decreases the variance). We have seen in the slides that variance decrease linearly with the number of samples for a random variable. In our case of importance smapling, if we increase the number of light ray than we are going to decrease the variance and therefore reduce noise in images.</p>
<p>The formula of variance to show that increasing the number of samples will reduce noise is the following (because variance decreases linearly with the number of samples):</p>
\[
V[\frac{1}{N}\sum_{i=1}^NY_i]=\frac{1}{N^2}\sum_{i=1}^NV[Y_i]=\frac{1}{N^2}NV[Y]=\frac{V[Y]}{N} 
\]


<h2 align="middle">Task IV: Global Illumination</h2>
<p>In this part we will implement full global illumination. In other words, we will need to fill the functions <code>est_radiance_global_illumination</code>, <code>zero_bounce_radiance</code>, <code>one_bounce_radiance</code>, and <code>at_least_one_bounce_radiance</code>.</p>
<p>For <code>zero_bounce_radiance</code>, we simply need to use the <code>get_emission</code> function to get the light emitted from the intersecting point.</p>
<p>For <code>one_bounce_radiance</code>, we simply call one of the direct illumination functions depending on the value of <code>direct_hemisphere_sample</code>. This function will be used to calculate the light generetated directly by the rays.</p>
<p>For <code>at_least_one_bounce_radiance</code>, we need to calculate the indirect rays by sampling the BSDF, performing Russian roulette step, and returning a recursively traced ray when applicable.</p>
<p>The <code>at_least_one_bounce_radiance</code> function takes as arguments a ray and an intersection. We first transform the world coordinates into local coordinate. We then take a sample from the surface BSDF at the intersection point using <code>isect.bsdf->sample_f()</code>. We can now do the russian roulette in which we decide to randomly stop a ray depending on the probability $p$. If we terminate the ray we just return an empty spectrum. If we do not stop it with the russian roulette we check if the depth of the ray is higher than 1. If it is higher than 1 than we generate a ray that has a depth that is 1 less than the incoming ray. If there is an intersection, then we accumulate the radiance in the <code>L_out</code> variable. The radiance is calculated by recursively calling the <code>at_least_one_bounce_radiance</code> function and multiplying it by the bsdf and the cosine of the angle between the normal vector and the incoming ray. From the slides we then divide that number by the pdf and the russian roulette probability. If the <code>max_ray_depth</code> is greater than 1 then we always try to trace the first ray without looking at the russian roulette probability.</p>

<p>The following is the <code>dae/sky/CBspheres_lambertian.dae</code> scene with only direct and only indirect illumination:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/spheresDirect.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 73.8418s with only direct illumination.</figcaption>
      </td>
      <td>
        <img src="images/spheresIndirect.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 447.6228ss with only indirect illumination.</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>We can see that the scene with only the direct rays have some colors that are very sharp, while the indirect lighting image produces color bleeding. The shadows in the indirect lighting scene are also much smoother and smaller. The indirect light is also much more pixelated as we are using the russian roulette in which the rays bounces everywhere in the scene and stops at a random depth. We can also see that the top of the spheres are black in the indirect scene as the light from the top of the sphere is due to the direct light.</p>

<p>The following is the same scene <code>dae/sky/CBspheres_lambertian.dae</code> with both direct and indirect lighting:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/spheresAll.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 433.5837s with direct and indirect illumination.</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>The combined scene renders the image of both the direct rays and indirect rays. The combined scene is as a result much brighter.</p>


<p>Here is the <code>CBbunny.dae</code> rendered views with <code>max_ray_depth</code> equal to 0, 1, 2, 3, and 100 (the <code>-m</code> flag).</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/CBbunnyPart40.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered with <code>max_ray_depth=0</code> in 540.5301s.</figcaption>
      </td>
      <td>
        <img src="images/CBbunnyPart41.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered with <code>max_ray_depth=1</code> in 385.4155s.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/CBbunnyPart42.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered with <code>max_ray_depth=2</code> in 387.5450s.</figcaption>
      </td>
      <td>
        <img src="images/CBbunnyPart43.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered with <code>max_ray_depth=3</code> in 492.9880s.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/CBbunnyPart4100.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered with <code>max_ray_depth=100</code> in 561.0243s.</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>When varying the max ray depth, the images generated are very similar. I am having difficulties see any differences between the generated images. Increasing the max ray depth will increase the amount of time needed to render an image as more rays will be drawn in the scene. By looking very closely at the images, some of the shadows may also be more smooth. This may be due to the fact that rays are bouncing more and therefore covering a larger area in the scene.</p>

<p>I then picked the <code>dae/sky/CBspheres_lambertian.dae</code> scene and rendered it with various sample-per-pixel rates, including 1, 2, 4, 8, 16, 64, and 1024, using 4 light rays:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/4spheres1_4.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 2.8027s with 1 sample-per-pixel.</figcaption>
      </td>
      <td>
        <img src="images/4spheres2_4.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 5.6261s with 2 sample-per-pixel.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/4spheres4_4.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 10.1463s with 4 sample-per-pixel.</figcaption>
      </td>
      <td>
        <img src="images/4spheres8_4.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 20.2302s with 8 sample-per-pixel.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/4spheres16_4.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 42.5675s with 16 sample-per-pixel.</figcaption>
      </td>
      <td>
        <img src="images/4spheres64_4.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 165.3106s with 64 sample-per-pixel.</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/4spheres1024_4.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBspheres_lambertian.dae</code> rendered in 2622.0377s with 1024 sample-per-pixel.</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>We can see that by increasing the sample-per-pixel the images get a lot less noisy then before. As mentioned above this is due to the fact that increasing the number of samples reduces the variance and as a result reduces the noise.</p>

<h2 align="middle">Task V: Adaptive Sampling</h2>
<p>In this part we are going to implement adaptive sampling to reduce noise without having to use a fixed high number of samples per pixel. Adaptive sampling changes the sample rate based on whether or not the pixel has converged as we trace rays through it. The pixel's convergence is calculated by the following formula:</p>
\[
I=1.96 \cdot \frac{\sigma}{\sqrt{n}}
\]
<p>We know that $I$ is small when the sample variance is small. We conclude that the pixel has converged when the following equation is met:</p>
\[
I \leq maxTolerance \cdot \mu
\]
<p>In my code the accumulated sample illuminance is tracked in the $s_1$ and $s_2$ variables. $s_1$ and $s_2$ are defined as following:</p>
\[
s_1=\sum_{k=1}^nx_k
\]
\[
s_2=\sum_{k=1}^nx_k^2
\]
<p>At each iteration, I am calculating the average using the formula:</p>
\[
\mu = \frac{s_1}{n}
\]
<p>The variance is calulated using the formula:</p>
\[
\sigma^2=\frac{1}{n-1}\cdot(s_2-\frac{s_1^2}{n})
\]
<p>We check for the pixel convergence for every $samplesPerBatch$ number of samples that we iterate to make the code more efficient.</p>

<p>The sample rate image is computed by filling the variable $sampleCountBuffer$ with the number of samples generated for each pixel. This is a 1D array so the position of a pixel with coordinates $x$ and $y$ in the array is <code>x + (y * sampleBuffer.w)</code>.</p>

<p>The following image is <code>dae/sky/CBbunny.dae</code> rendered with 2048 samples per pixel, 1 sample per light, and 5 for max ray depth using adaptive sampling:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/bunnyAdap.png" align="middle" width="400px"/>
        <figcaption align="middle"><code>dae/sky/CBbunny.dae</code> rendered with in 1450.5718s.</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>The sample rate of each pixel for the above image is the following:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/bunnyAdap_rate.png" align="middle" width="400px"/>
        <figcaption align="middle">Sample rate for adaptive sampling of <code>dae/sky/CBbunny.dae</code>.</figcaption>
      </td>
    </tr>
  </table>
</div>
</body>
</html>
